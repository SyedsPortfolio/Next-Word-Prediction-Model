{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxmT+IoPJkbJRCltXFh9LU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyedsProjectPortfolio45/Next-Word-Prediction-Model/blob/main/Next_Word_prediction_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLeqMmRwJ40O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Read the text file\n",
        "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let’s tokenize the text to create a sequence of words:"
      ],
      "metadata": {
        "id": "LWtDh48RbVOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "sfJoEInjKrjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process.\n",
        "The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input.\n",
        "This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index.\n",
        "The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text.'''"
      ],
      "metadata": {
        "id": "DgST5aUrbfir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let’s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences\n",
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "dXtb0GU-K4v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''In the above code, the text data is split into lines using the ‘\\n’ character as a delimiter.\n",
        "For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary.\n",
        "The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’.\n",
        "\n",
        "This n-gram sequence represents the input context, with the last token being the target or predicted word. This n-gram sequence is then appended to the ‘input_sequences’ list.\n",
        "This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model.'''\n",
        "\n",
        "#Now let’s pad the input sequences to have equal length:"
      ],
      "metadata": {
        "id": "Y-ZiNfHJb2Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ],
      "metadata": {
        "id": "MG1qlj_DLGps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''In the above code, the input sequences are padded to ensure all sequences have the same length.\n",
        "The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences.\n",
        "The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n",
        "\n",
        "The ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument.\n",
        "Finally, the input sequences are converted into a numpy array to facilitate further processing.'''\n",
        "\n",
        "#Now let’s split the sequences into input and output:"
      ],
      "metadata": {
        "id": "IssAMO0zcFf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "9r8NN5IELO4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ],
      "metadata": {
        "id": "_tyIn8l5LSxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKa7n28DLgIS",
        "outputId": "30aad2c4-6721-42ac-862b-6bf24a75f9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2208800 (8.43 MB)\n",
            "Trainable params: 2208800 (8.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''The code above defines the model architecture for the next word prediction model. The ‘Sequential’ model is created, which represents a linear stack of layers.\n",
        "The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
        "\n",
        "1.‘total_words’, which represents the total number of distinct words in the vocabulary;\n",
        "2.‘100’, which denotes the dimensionality of the word embeddings;\n",
        "3.and ‘input_length’, which specifies the length of the input sequences.\n",
        "\n",
        "The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data.\n",
        "It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
        "\n",
        "Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions.\n",
        "It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.'''\n",
        "\n",
        "#Now let’s compile and train the model:"
      ],
      "metadata": {
        "id": "39mng3rJc9vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkZHAgeQLjaB",
        "outputId": "2c170b66-211c-4654-a43c-08655a5735e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 191s 63ms/step - loss: 6.2300 - accuracy: 0.0779\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 5.5011 - accuracy: 0.1250\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 5.1276 - accuracy: 0.1462\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 4.8068 - accuracy: 0.1642\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 4.5073 - accuracy: 0.1805\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 4.2228 - accuracy: 0.2003\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 3.9509 - accuracy: 0.2253\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 3.6940 - accuracy: 0.2525\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 3.4516 - accuracy: 0.2858\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 3.2274 - accuracy: 0.3208\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 3.0184 - accuracy: 0.3557\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 2.8254 - accuracy: 0.3895\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 2.6471 - accuracy: 0.4235\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 2.4815 - accuracy: 0.4549\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 2.3296 - accuracy: 0.4868\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 2.1897 - accuracy: 0.5134\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 2.0626 - accuracy: 0.5385\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 1.9425 - accuracy: 0.5659\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 1.8353 - accuracy: 0.5884\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 1.7353 - accuracy: 0.6113\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 191s 63ms/step - loss: 1.6420 - accuracy: 0.6287\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 1.5574 - accuracy: 0.6482\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 1.4775 - accuracy: 0.6653\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 1.4058 - accuracy: 0.6819\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 1.3411 - accuracy: 0.6958\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 1.2784 - accuracy: 0.7103\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 1.2206 - accuracy: 0.7226\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 1.1700 - accuracy: 0.7335\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 1.1205 - accuracy: 0.7453\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 1.0785 - accuracy: 0.7541\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 1.0364 - accuracy: 0.7627\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.9999 - accuracy: 0.7713\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.9644 - accuracy: 0.7813\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 0.9322 - accuracy: 0.7862\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.9063 - accuracy: 0.7920\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.8762 - accuracy: 0.7972\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.8513 - accuracy: 0.8042\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.8285 - accuracy: 0.8090\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.8072 - accuracy: 0.8131\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.7904 - accuracy: 0.8162\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 0.7703 - accuracy: 0.8202\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.7506 - accuracy: 0.8251\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.7332 - accuracy: 0.8287\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.7228 - accuracy: 0.8304\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 0.7085 - accuracy: 0.8336\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.6972 - accuracy: 0.8368\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6842 - accuracy: 0.8379\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6743 - accuracy: 0.8386\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6682 - accuracy: 0.8398\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 188s 63ms/step - loss: 0.6539 - accuracy: 0.8438\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6465 - accuracy: 0.8455\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.6412 - accuracy: 0.8458\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6317 - accuracy: 0.8470\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.6197 - accuracy: 0.8510\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 185s 62ms/step - loss: 0.6185 - accuracy: 0.8497\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.6139 - accuracy: 0.8504\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.6068 - accuracy: 0.8521\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.6008 - accuracy: 0.8527\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.5955 - accuracy: 0.8536\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5910 - accuracy: 0.8546\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5833 - accuracy: 0.8572\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.5878 - accuracy: 0.8548\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5811 - accuracy: 0.8552\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5727 - accuracy: 0.8579\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5790 - accuracy: 0.8553\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5653 - accuracy: 0.8578\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 0.5629 - accuracy: 0.8593\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5608 - accuracy: 0.8601\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5568 - accuracy: 0.8601\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5593 - accuracy: 0.8597\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 188s 62ms/step - loss: 0.5564 - accuracy: 0.8591\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5532 - accuracy: 0.8597\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5483 - accuracy: 0.8609\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5479 - accuracy: 0.8610\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 0.5474 - accuracy: 0.8605\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 0.5463 - accuracy: 0.8602\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 0.5483 - accuracy: 0.8602\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5406 - accuracy: 0.8608\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5387 - accuracy: 0.8625\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5457 - accuracy: 0.8589\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5382 - accuracy: 0.8612\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 190s 63ms/step - loss: 0.5313 - accuracy: 0.8634\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5356 - accuracy: 0.8617\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 189s 63ms/step - loss: 0.5334 - accuracy: 0.8619\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 187s 62ms/step - loss: 0.5317 - accuracy: 0.8622\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5316 - accuracy: 0.8616\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5308 - accuracy: 0.8617\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5295 - accuracy: 0.8625\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5249 - accuracy: 0.8636\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5287 - accuracy: 0.8624\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5285 - accuracy: 0.8621\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 184s 61ms/step - loss: 0.5190 - accuracy: 0.8652\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 185s 62ms/step - loss: 0.5209 - accuracy: 0.8639\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5235 - accuracy: 0.8627\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 185s 61ms/step - loss: 0.5192 - accuracy: 0.8639\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 185s 62ms/step - loss: 0.5185 - accuracy: 0.8642\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 185s 62ms/step - loss: 0.5184 - accuracy: 0.8645\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5223 - accuracy: 0.8625\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 185s 62ms/step - loss: 0.5175 - accuracy: 0.8631\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 186s 62ms/step - loss: 0.5194 - accuracy: 0.8629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79aa2a851090>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''The ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training.\n",
        "After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’.\n",
        "The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset. The ‘verbose’ parameter is set to ‘1’ to display the training process.\n",
        "'''\n",
        "###The above code will take more than an hour to execute."
      ],
      "metadata": {
        "id": "SQdCY_Aqdf-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"I will leave if they\"\n",
        "next_words = 4\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "ocuz4u4_LwML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a018578-dcbf-4a6d-b767-feebaa06f701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "I will leave if they should name the police\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''The above code generates the next word predictions based on a given seed text. The ‘seed_text’ variable holds the initial text.\n",
        "The ‘next_words’ variable determines the number of predictions to be generated. Inside the for loop, the ‘seed_text’ is converted into a sequence of tokens using the tokenizer.\n",
        "The token sequence is padded to match the maximum sequence length.\n",
        "\n",
        "The model predicts the next word by calling the ‘predict’ method on the model with the padded token sequence.\n",
        "The predicted word is obtained by finding the word with the highest probability score using ‘np.argmax’.\n",
        "Then, the predicted word is appended to the ‘seed_text’, and the process is repeated for the desired number of ‘next_words’.\n",
        "Finally, the ‘seed_text’ is printed, which contains the initial text followed by the generated predictions.'''"
      ],
      "metadata": {
        "id": "Pf7VyIjnjZUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}